{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaae221a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6207134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib>=3.2.2 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (1.20.3)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (4.5.5.62)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (8.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (2.26.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (1.10.2)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (0.11.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (4.62.3)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (2.8.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 20)) (1.3.4)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 21)) (0.11.2)\n",
      "Requirement already satisfied: thop in d:\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 37)) (0.0.31.post2005241907)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda3\\lib\\site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda3\\lib\\site-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (3.10.0.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from tqdm>=4.41.0->-r requirements.txt (line 13)) (0.4.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.19.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (2.6.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.44.0)\n",
      "Requirement already satisfied: wheel>=0.26 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.37.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.3.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (58.0.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in d:\\anaconda3\\lib\\site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (2.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 20)) (2021.3)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\lib\\site-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!cd yolov5 & pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee17971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc138800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to C:\\Users\\Gokul/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2022-3-7 torch 1.10.2+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "!cd yolov5\n",
    "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path='yolov5/best.pt', force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1042fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edabf6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Input, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c4098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8405ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_func(img):\n",
    "    #Step 1: contours\n",
    "    cnts = cv2.findContours(img.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "    cnts=sorted(cnts, key = cv2.contourArea, reverse = True)[:30] #sort contours based on their area keeping minimum required area as '30' (anything smaller than this will not be considered)\n",
    "    NumberPlateCnt = None #we currently have no Number plate contour\n",
    "\n",
    "    # loop over our contours to find the best possible approximate contour of number plate\n",
    "    count = 0\n",
    "    for c in cnts:\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "            if len(approx) == 4:  # Select the contour with 4 corners\n",
    "                NumberPlateCnt = approx #This is our approx Number Plate Contour\n",
    "                x,y,w,h = cv2.boundingRect(c)\n",
    "                ROI = img[y:y+h, x:x+w]\n",
    "                break\n",
    "\n",
    "    if NumberPlateCnt is not None:\n",
    "        # Drawing the selected contour on the original image\n",
    "        cv2.drawContours(img, [NumberPlateCnt], -1, (0,255,0), 3)\n",
    "        \n",
    "#     print(NumberPlateCnt)\n",
    "    #Step 2: rotation\n",
    "    \n",
    "    \n",
    "    # Distance between (x1, y1) and (x2, y2)\n",
    "    def dist(x1, x2, y1, y2):\n",
    "        return ((x1-x2)**2+(y1-y2)**2)**0.5\n",
    "    \n",
    "    idx=0\n",
    "    m=1\n",
    "    # To find the index of coordinate with maximum y-coordinate\n",
    "    if NumberPlateCnt is not None:\n",
    "        for i in range(4):\n",
    "            if NumberPlateCnt[i][0][1]>m:\n",
    "                idx=i\n",
    "                m=NumberPlateCnt[i][0][1]\n",
    "\n",
    "        # Assign index to the previous coordinate\n",
    "        if idx==0:\n",
    "            pin=3\n",
    "        else:\n",
    "            pin=idx-1\n",
    "\n",
    "        # Assign index to the next coordinate\n",
    "        if idx==3:\n",
    "            nin=0\n",
    "        else:\n",
    "            nin=idx+1\n",
    "\n",
    "        # Find distances between the acquired coordinate and its previous and next coordinate\n",
    "        p=dist(NumberPlateCnt[idx][0][0], NumberPlateCnt[pin][0][0], NumberPlateCnt[idx][0][1], NumberPlateCnt[pin][0][1])\n",
    "        n=dist(NumberPlateCnt[idx][0][0], NumberPlateCnt[nin][0][0], NumberPlateCnt[idx][0][1], NumberPlateCnt[nin][0][1])\n",
    "\n",
    "        # The coordinate that has more distance from the acquired coordinate is the required second bottom-most coordinate\n",
    "        if p>n:\n",
    "            if NumberPlateCnt[pin][0][0]<NumberPlateCnt[idx][0][0]:\n",
    "                left=pin\n",
    "                right=idx\n",
    "            else:\n",
    "                left=idx\n",
    "                right=pin\n",
    "            d=p\n",
    "        else:\n",
    "            if NumberPlateCnt[nin][0][0]<NumberPlateCnt[idx][0][0]:\n",
    "                left=nin\n",
    "                right=idx\n",
    "            else:\n",
    "                left=idx\n",
    "                right=nin\n",
    "            d=n\n",
    "\n",
    "        left_x=NumberPlateCnt[left][0][0]\n",
    "        left_y=NumberPlateCnt[left][0][1]\n",
    "        right_x=NumberPlateCnt[right][0][0]\n",
    "        right_y=NumberPlateCnt[right][0][1]\n",
    "    #     print(left_x, left_y, right_x, right_y)\n",
    "\n",
    "        # Finding the angle of rotation by calculating sin of theta\n",
    "        opp=right_y-left_y\n",
    "        hyp=((left_x-right_x)**2+(left_y-right_y)**2)**0.5\n",
    "        sin=opp/hyp\n",
    "        theta=math.asin(sin)*57.2958\n",
    "\n",
    "        # Rotate the image according to the angle of rotation obtained\n",
    "        image_center = tuple(np.array(ROI.shape[1::-1]) / 2)\n",
    "        rot_mat = cv2.getRotationMatrix2D(image_center, theta, 1.0)\n",
    "        result = cv2.warpAffine(ROI, rot_mat, ROI.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "\n",
    "        # The image can be cropped after rotation( since rotated image takes much more height)\n",
    "        if opp>0:\n",
    "            h=result.shape[0]-opp//2\n",
    "        else:\n",
    "            h=result.shape[0]+opp//2\n",
    "\n",
    "        result=result[0:h, :]\n",
    "    #     plt.imshow(result)\n",
    "    #     plt.title(\"Plate obtained after rotation\")\n",
    "    #     plt.show()\n",
    "        cv2.imshow(\"Plate obtained after rotation\", result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ca0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contours(dimensions, img) :\n",
    "\n",
    "    # Find all contours in the image\n",
    "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve potential dimensions\n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "    \n",
    "    # Check largest 5 or  15 contours for license plate or character respectively\n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "    \n",
    "    ii = img\n",
    "    \n",
    "    x_cntr_list = []\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    for cntr in cntrs :\n",
    "        # detects contour in binary image and returns the coordinates of rectangle enclosing it\n",
    "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
    "        \n",
    "        # checking the dimensions of the contour to filter out the characters by contour's size\n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
    "\n",
    "            char_copy = np.zeros((44,24))\n",
    "            # extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "            \n",
    "            cv2.rectangle(ii, (intX,intY), (intWidth+intX, intY+intHeight), (50,21,200), 2)\n",
    "#             plt.imshow(ii, cmap='gray')\n",
    "#             plt.title('Predict Segments')\n",
    "            cv2.imshow('prediction', ii)\n",
    "\n",
    "            # Make result formatted for classification: invert colors\n",
    "            char = cv2.subtract(255, char)\n",
    "\n",
    "            # Resize the image to 24x44 with black border\n",
    "            char_copy[2:42, 2:22] = char\n",
    "            char_copy[0:2, :] = 0\n",
    "            char_copy[:, 0:2] = 0\n",
    "            char_copy[42:44, :] = 0\n",
    "            char_copy[:, 22:24] = 0\n",
    "\n",
    "            img_res.append(char_copy) # List that stores the character's binary image (unsorted)\n",
    "            \n",
    "    # Return characters on ascending order with respect to the x-coordinate (most-left character first)\n",
    "            \n",
    "    plt.show()\n",
    "    # arbitrary function that stores sorted list of character indeces\n",
    "    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])\n",
    "    img_res_copy = []\n",
    "    for idx in indices:\n",
    "        img_res_copy.append(img_res[idx])# stores character images according to their index\n",
    "    img_res = np.array(img_res_copy)\n",
    "\n",
    "    return img_res\n",
    "\n",
    "def segment_characters(img_binary_lp) :\n",
    "    if img_binary_lp is not None:\n",
    "        LP_WIDTH = img_binary_lp.shape[0]\n",
    "        LP_HEIGHT = img_binary_lp.shape[1]\n",
    "\n",
    "        # Make borders white\n",
    "        img_binary_lp[0:3,:] = 255\n",
    "        img_binary_lp[:,0:3] = 255\n",
    "        img_binary_lp[72:75,:] = 255\n",
    "        img_binary_lp[:,330:333] = 255\n",
    "\n",
    "        # Estimations of character contours sizes of cropped license plates\n",
    "        dimensions = [LP_WIDTH/6,\n",
    "                           LP_WIDTH/2,\n",
    "                           LP_HEIGHT/10,\n",
    "                           2*LP_HEIGHT/3]\n",
    "        cv2.imwrite('contour.jpg',img_binary_lp)\n",
    "\n",
    "        # Get contours within cropped license plate\n",
    "        char_list = find_contours(dimensions, img_binary_lp)\n",
    "\n",
    "        return char_list\n",
    "\n",
    "# Predicting the output\n",
    "def fix_dimension(img): \n",
    "    new_img = np.zeros((28,28,3))\n",
    "    for i in range(3):\n",
    "        new_img[:,:,i] = img\n",
    "        return new_img\n",
    "  \n",
    "def show_results(char):\n",
    "    dic = {}\n",
    "    characters = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "    for i,c in enumerate(characters):\n",
    "        dic[i] = c\n",
    "\n",
    "    output = []\n",
    "    for i,ch in enumerate(char): #iterating over the characters\n",
    "        img_ = cv2.resize(ch, (28,28), interpolation=cv2.INTER_AREA)\n",
    "        img = fix_dimension(img_)\n",
    "        img = img.reshape(1,28,28,3) #preparing image for the model\n",
    "        predict_x=loaded_model.predict(img) \n",
    "        classes_x=np.argmax(predict_x) #predicting the class\n",
    "#         character = str(classes_x)\n",
    "        character = dic[classes_x]\n",
    "        output.append(character) #storing the result in a list\n",
    "        \n",
    "    plate_number = ''.join(output)\n",
    "    \n",
    "    return plate_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca8f47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingOCR(image):\n",
    "    #01: Resizing\n",
    "    if image is not None:\n",
    "        resize_image = cv2.resize(image, None, fx=10, fy=10, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        ## 02: Binarization \n",
    "        gray_image =  cv2.cvtColor(resize_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ## 03: Noise Removal\n",
    "        no_noise = cv2.bilateralFilter(gray_image, 11, 17, 17)\n",
    "\n",
    "        #OG OG OG OG OG OG\n",
    "    #     def noise_removal(image):\n",
    "    #         import numpy as np\n",
    "    #         kernel = np.ones((1, 1), np.uint8)\n",
    "    #         image = cv2.dilate(image, kernel, iterations=1)\n",
    "    #         kernel = np.ones((1, 1), np.uint8)\n",
    "    #         image = cv2.erode(image, kernel, iterations=1)\n",
    "    #         image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "    #         image = cv2.medianBlur(image, 3)\n",
    "    #         return (image)\n",
    "\n",
    "    #     no_noise = noise_removal(gray_image)\n",
    "        #OG OG OG OG OG OG\n",
    "\n",
    "\n",
    "        ### 04: CLAHE\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        cl1 = clahe.apply(no_noise)\n",
    "\n",
    "        ### 05: Contrast\n",
    "        def contrasts(image):\n",
    "            alpha = 1.5 # Contrast control (1.0-3.0)\n",
    "            beta = 20 # Brightness control (0-100)\n",
    "\n",
    "            return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "        contrasted_image = contrasts(cl1)\n",
    "\n",
    "        ### 06: Erosion\n",
    "        def thin_font(image):\n",
    "            import numpy as np\n",
    "            image = cv2.bitwise_not(image)\n",
    "            kernel = np.ones((2,2),np.uint8)\n",
    "            image = cv2.erode(image, kernel, iterations=1)\n",
    "            image = cv2.bitwise_not(image)\n",
    "            return (image)\n",
    "\n",
    "        eroded_image = thin_font(contrasted_image)\n",
    "\n",
    "    #     def thick_font(image):\n",
    "    #         import numpy as np\n",
    "    #         image = cv2.bitwise_not(image)\n",
    "    #         kernel = np.ones((2,2),np.uint8)\n",
    "    #         image = cv2.dilate(image, kernel, iterations=1)\n",
    "    #         image = cv2.bitwise_not(image)\n",
    "    #         return (image)\n",
    "\n",
    "    #     dilated_image = thick_font(eroded_image)\n",
    "    #     ax[1,2].imshow(dilated_image)\n",
    "    #     ax[1,2].set_title('dilated_image')\n",
    "\n",
    "        ### 07: Threshold\n",
    "        thresh = cv2.threshold(eroded_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "        ### 08: Rotation and segmentation\n",
    "\n",
    "        result = rotation_func(thresh)\n",
    "\n",
    "        char_list = segment_characters(result)\n",
    "        if char_list is not None:\n",
    "            plate_no = show_results(char_list)\n",
    "            print(plate_no)\n",
    "\n",
    "    #     ### 08: Remove Border\n",
    "    #     def remove_borders(image):\n",
    "    #         contours, heiarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #         cntsSorted = sorted(contours, key=lambda x:cv2.contourArea(x))\n",
    "    #         cnt = cntsSorted[-1]\n",
    "    #         x, y, w, h = cv2.boundingRect(cnt)\n",
    "    #         crop = image[y:y+h, x:x+w]\n",
    "    #         return (crop)\n",
    "\n",
    "    #     no_borders = remove_borders(thresh)\n",
    "        cv2.imshow('LP', thresh)\n",
    "    #     cv2.putText(image, plate_no, (50,50), cv2.FONT_ITALIC, 0.6, (0, 255, 0), 1)\n",
    "    #     plt.imshow(no_borders)\n",
    "\n",
    "    #     ax[1,4].imshow(no_borders)\n",
    "    #     ax[1,4].set_title('Processed - Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b9ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "loaded_model = keras.models.load_model('my_char_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9458f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UP6CXZ89S\n",
      "UP6CXZ89S\n",
      "UP6CXZ89S\n",
      "UP6CXZ89S\n",
      "U6CXZ89S\n",
      "U6CI28SS\n",
      "U6CXZ8S5\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11184/511629893.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mY2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ymax'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mroi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mprocessingOCR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;31m# cv2.imshow(\"License Plate\", roi)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# crops = results.crop(save=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11184/2187239881.py\u001b[0m in \u001b[0;36mprocessingOCR\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#01: Resizing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mresize_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m## 02: Binarization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('supercars_Trim.mp4')\n",
    "# frames_counter = 1\n",
    "\n",
    "# def rescale_frame(frame, percent=50):\n",
    "#     width = int(frame.shape[1] * percent/ 100)\n",
    "#     height = int(frame.shape[0] * percent/ 100)\n",
    "#     dim = (width, height)\n",
    "#     return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "#     frames_counter = frames_counter + 1\n",
    "    \n",
    "#     rescale = rescale_frame(frame)\n",
    "    # Make detections \n",
    "    results = yolo_model(frame)\n",
    "#     results = yolo_model(rescale)q\n",
    "    res = results.pandas().xyxy[0].sort_values('xmin')\n",
    "    # print(!res.empty)\n",
    "    if not (res.empty) and res.loc[0,'confidence'] >=0.70:\n",
    "        # print(\"Current Frame has LP and atleast one confidence is high enough\")\n",
    "        # print(res)\n",
    "        X1 = int(res.loc[0,'xmin'])\n",
    "        Y1 = int(res.loc[0,'ymin'])\n",
    "        X2 = int(res.loc[0,'xmax'])\n",
    "        Y2 = int(res.loc[0,'ymax'])\n",
    "        roi = frame[Y1-5:Y2+5, X1-5:X2+5]\n",
    "        processingOCR(roi)\n",
    "        # cv2.imshow(\"License Plate\", roi)\n",
    "        # crops = results.crop(save=False)\n",
    "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# print(\"Number of frames in the video: \", frames_counter)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e934d9",
   "metadata": {},
   "source": [
    "## CNN character recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd5364c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 images belonging to 36 classes.\n",
      "Found 216 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, width_shift_range=0.1, height_shift_range=0.1)\n",
    "path = 'data/data'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        path+'/train',  # this is the target directory\n",
    "        target_size=(28,28),  # all images will be resized to 28x28\n",
    "        batch_size=1,\n",
    "        class_mode='sparse')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        path+'/val',  # this is the target directory\n",
    "        target_size=(28,28),  # all images will be resized to 28x28 batch_size=1,\n",
    "        class_mode='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37407f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (22,22), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (16,16), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (8,8), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (4,4), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(36, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizers.Adam(lr=0.0001), metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "960b4fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 16)        23248     \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        131104    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 64)        131136    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        65600     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 7, 7, 64)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               401536    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 36)                4644      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 757,268\n",
      "Trainable params: 757,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0442f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "219/864 [======>.......................] - ETA: 19s - loss: 3.5843 - accuracy: 0.0457"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8564/3977876835.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m result = model.fit(\n\u001b[0m\u001b[0;32m      3\u001b[0m       \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "result = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch = train_generator.samples // batch_size,\n",
    "      validation_data = validation_generator, \n",
    "      epochs = 25, verbose=1, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_char_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ab816",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,5))\n",
    "grid=gridspec.GridSpec(ncols=2,nrows=1,figure=fig)\n",
    "fig.add_subplot(grid[0])\n",
    "plt.plot(result.history['accuracy'], label='training accuracy')\n",
    "plt.plot(result.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "\n",
    "fig.add_subplot(grid[1])\n",
    "plt.plot(result.history['loss'], label='training loss')\n",
    "plt.plot(result.history['val_loss'], label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab37898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model instance\n",
    "loaded_model = Sequential()\n",
    "loaded_model.add(Conv2D(16, (22,22), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(Conv2D(32, (16,16), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(Conv2D(64, (8,8), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(Conv2D(64, (4,4), input_shape=(28, 28, 3), activation='relu', padding='same'))\n",
    "loaded_model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "loaded_model.add(Dropout(0.4))\n",
    "loaded_model.add(Flatten())\n",
    "loaded_model.add(Dense(128, activation='relu'))\n",
    "loaded_model.add(Dense(36, activation='softmax'))\n",
    "\n",
    "# Restore the weights\n",
    "loaded_model.load_weights('checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8bda8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
